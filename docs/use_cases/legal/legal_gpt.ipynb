{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6488fdaf",
   "metadata": {},
   "source": [
    "# 法律助理\n",
    "\n",
    "测试和实验GPT3.5在中国法律助理场景的应用：对中国法律的理解，对法律条款和陈述的综合分析能力的测试。\n",
    "尝试利用以前bar考的题库建立一个测试集，在其基础上对基础模型的能力建立比较准确的认知。后面可以体系性的开发提效。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c29ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acb4a2f6",
   "metadata": {},
   "source": [
    "构建框架：数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b18bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"truthful_qa\", \"generation\")\n",
    "#UTF8 Comma delimited CSV file exported from exel\n",
    "dataset = load_dataset(\"csv\", data_files=\"/mnt/c/Users/qiang/Downloads/BarExamData_CSV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c431aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = list(dataset['train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "GPT4='GPT-4-32k'\n",
    "GPT3_5='text-davinci-003'\n",
    "llm = OpenAI(model_name=GPT3_5, temperature=0.0, max_tokens=1500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b23793a",
   "metadata": {},
   "source": [
    "最简单的推理测试：找到好的prompt很不容易，需要对法律场景、问题类型、模型与人在这样问题上的思维方式及其偏见（inductive bias）有深刻的理解后（经过很多尝试后）才能生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312aa79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qTemplate=\"请仔细理解以下法律条款和陈述后回答问题 法律条款：{law} 陈述：{prefix} {statement} 问题：{question}\"\n",
    "\n",
    "question = \"以上法律条款适用于以上陈述的先决条件是什么？这个条件满足了吗？为什么？陈述是正确的吗？为什么？\"\n",
    "\n",
    "qPrompt = PromptTemplate(input_variables=[\"law\",\"prefix\", \"statement\", \"question\"], template=qTemplate)\n",
    "qChain = LLMChain(llm=llm, verbose=True, prompt=qPrompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3453ce2",
   "metadata": {},
   "source": [
    "Add the question prompt to each data entry which can be modified independent of input data.\n",
    "Evaluating all the data at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706dc2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_add(list_):\n",
    "    yield from ({'question':question, 'law':entry['law'], 'prefix':entry['prefix'], 'statement':entry['statement'], 'answer':entry['answer']} for entry in list_)\n",
    "\n",
    "# Prints all results in once\n",
    "eles = list(generator_add(examples))\n",
    "\n",
    "predictions = qChain.apply(eles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cee1826",
   "metadata": {},
   "source": [
    "把上面复杂、细节的回答转换为简单的答案（正确或错误）以便最后评估结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "aTemplate=\"{text} 问题：陈述是正确的还是错误的（请回答正确或错误）？\"\n",
    "\n",
    "aPrompt = PromptTemplate(input_variables=[\"text\"], template=aTemplate)\n",
    "aChain = LLMChain(llm=llm, verbose=True, prompt=aPrompt)\n",
    "\n",
    "predictionsAns = aChain.apply(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0234ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsAns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5d233f8",
   "metadata": {},
   "source": [
    "Manual Evaluate/Probe one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#i=1\n",
    "#qChain.run(law=eles[i]['law'], prefix=eles[i]['prefix'], statement=eles[i]['statement'], question=eles[i]['question'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28442222",
   "metadata": {},
   "source": [
    "## Evaluate performance\n",
    "Now we can evaluate the predictions. The first thing we can do is look at them by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6788b1a5",
   "metadata": {},
   "source": [
    "Next, we can use a language model to score them programatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs = eval_chain.evaluate(eles, predictionsAns, question_key=\"statement\", prediction_key=\"text\", answer_key='answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded_outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51f5b9ec",
   "metadata": {},
   "source": [
    "We can add in the graded output to the `predictions` dict and then get a count of the grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28963060",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, prediction in enumerate(predictionsAns):\n",
    "    prediction['grade'] = graded_outputs[i]['text']\n",
    "    prediction['answer'] = eles[i]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24220d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter([pred['grade'] for pred in predictionsAns])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c407540",
   "metadata": {},
   "source": [
    "We can also filter the datapoints to the incorrect examples and look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb22638",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = [pred for pred in predictionsAns if pred['grade'] == \" INCORRECT\"]\n",
    "correct = [pred for pred in predictionsAns if pred['grade'] == \" CORRECT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e41549",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
